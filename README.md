# Description

A fictitious Retail customer loyalty application demo for single region and multi-region on AWS

This demo simulates a retail customer loyalty application workload of customers getting loyalty credits for their retail purchases.  The dbworkload Python simulation does this via 2 transactions, one to find the customer id for the given customer, and another to update the customer’s loyalty credits.

We start with a single region database simulation, using dbworkload, of performance for both customers located in the same region as the cluster and customers located in a different region from the cluster.  Performance for the local customers will, of course, be better than for the remote customers.

We then move onto a demo of a multi-regional cluster simulation using regional by rows tables and with dbworkload workloads running in each region to show that performance is the same for all users regardless of which region the user is located in.  This allows us to show off CockroachDB’s geo-locality functionality.  You can also scale up the load for performance demos using dbworkload’s parallelism capability.

The demo uses 2 tables, a customers table and a loyalty table, and each has 30 million rows, which is generated by dbworkload.

Also for the single region demo we will manually create the AWS environment to get you familiar with using AWS and for the multi-region demo we will use Ron Nollen’s multi-region Terraform github repository to create the multi-region environment.

# Files

|  File|Description  |
|--|--|
|add_loyalty_keys_to_customers_table.sql  |SQL to add customers UUID key to loyalty table  |
|build_test_tables.sql  |SQL DDL commands to built 2 test tables only |
|customers.yaml  |yaml file to create customers data files for 30 million rows  |
|customers_test.csv  |5 rows of customers data to test import command |
|customers_test.yaml  |yaml file to create test customers data file for 5 rows |
|database_build_script.sql  |SQL DDL commands to build database objects |
|haproxy.cfg  |Single region HAProxy config file  |
|load_customers_aws.sql  |SQL commands to import all 30 million rows of customers data |
|load_customers_test_aws.sql  |SQL command to import 5 rows of customers test data to loyalty_test table |
|load_loyalty_aws.sql  |SQL commands to import all 30 million rows of loyalty data |
|load_loyalty_test_aws.sql  |SQL command to import 5 rows of loyalty test data to loyalty_test table |
|loyalty.yaml  |yaml file to create loyalty data files for 30 million rows  |
|loyalty_test.csv  |5 rows of loyalty data to test import command |
|loyalty_test.yaml  |yaml file to create test loyalty data file for 5 rows   |
|run_dbworkload.sh  |Bash script to kickoff dbworkload runs on the app node(s) |
|ulta_beauty.py  |Python program to run using dbworkload |

# Hardware Configurations

## Single region demo:

I have used a two level architecture for this demo with the database cluster at one level, and the application server nodes as the second level with the application server nodes running both the proxy server (load balancer) and the dbworkload workload.

* Cluster nodes config: 3 EC2 nodes using m6a.2xlarge instances (8 cpu’s and 32 GB RAM with 120GB SSD)
	Each node will be in a single region and it’s own zone (you must pick a region with a minimum of 3 zones)
* Application server nodes config: 2 EC2 nodes using m6a.2xlarge instances (8 cpu’s and 32 GB RAM with 100GB SSD)
	One node will be in the same region as the cluster and the other will be in a much farther away region

__My region choices were to create the cluster in us-east-2 (Ohio) and have the application server nodes in us-east-2 (Ohio) and us-west-2 (Oregon)__

## Multi-region demo:

Ron’s Terraform scripts create a three level architecture.  One level for the database cluster, one level for the proxy server nodes, and a third level for the application server nodes, where you will run the dbworkload workload.  You must have all three levels.  (I have tried to create a 2 level config and it doesn’t work.). When you setup the Terraform script’s configuration file, make sure you pick AWS regions with a minimum of 3 zones; i.e. do NOT use us-west-1 (Northern California).

* Cluster nodes config: 9 EC2 nodes using m6a.2xlarge instances
	There will be 3 regions and 3 nodes per region with each node in its own region.
	Ron’s script will automatically create these for you in the right zones and regions.
* Proxy nodes config: 3 EC2 nodes using m6a.xlarge instances
	Each proxy node will be in a separate region.  Ron’s script will automatically create these for you in the right regions.
* Application server nodes config: 3 EC2 nodes using m6a.2xlarge instances
	Each application node will be in a separate region.  Ron’s script will automatically create these for you in the right regions.

My region choices were to use us-east-1 (North Virginia), us-east-2 (Ohio) and us-west-2 (Oregon).

# Setup

Before you start I highly suggest you read Fabio's write up on using dbworkload at:

   [dbworkload Details](https://fabiog1901.github.io/dbworkload/)


## General Setup

1. Create key pairs for each region you are using for your single region and mult-region clusters.  Create 1 key pair for each of the 3 regions you’ll be using.  I would suggest us-east-1 (North Virginia), us-east-2 (Ohio) and us-west-2 (Oregon) .  Use the default of RSA and .pem if you are using a Mac and download each .pem file to your Mac for later use.

2. Create a non-public S3 bucket where you will put your table data files that you generate with dbworkload.  You’ll later import these data files into your tables using the SQL IMPORT INTO command.  I suggest you create the S3 bucket in the same region as the single region database cluster.  Note down which region your choose for later usage.  (Note: Fabio’s dbworkload github mentions doing this by using a web server on your application node.  With the latest versions of CRDB (I used version 24.2.2) this no longer will work if you have a large number of data files to import.)  Copy your bucket's URI; select the 'Copy S3 URI' button in the upper right corner of the S3 bucket screen.

3. Create an IAM policy to allow you to upload, download and access an S3 bucket.  After you press the 'create policy' button select the JSON tab on your upper right.  You'll want to use the JSON below as your template.  Only alter the Resource section.  You can see that I have 6 entries which are for 3 S3 buckets.  Remove my entries and create yours as follows: For each S3 URI, which you copied earlier, create 2 entries; one that ends with a __/*__ and one that doesn't.  Make sure each entry starts with __arn:aws:__  And don't forget a comma at the end of each entry except for the last one.  Then click the 'Next' button in the lower right, enter your policy name, and create your policy.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::adam-london",
                "arn:aws:s3:::adam-london/*",
                "arn:aws:s3:::adam-london/demo/loyalty",
                "arn:aws:s3:::adam-london/demo/loyalty/*",
                "arn:aws:s3:::adam-london/demo/customer",
                "arn:aws:s3:::adam-london/demo/customer/*"
            ]
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": "s3:ListAllMyBuckets",
            "Resource": "*"
        }
    ]
}
```

4. Create an IAM user and access key.  Got to the IAM users screen and select the create user button.  When you get to the 'Set Permissions' screen, select the 'attach policies directly' choice and then select the policy you just created.  Then select the 'Next' button and create the user.

Now go back to the list of IAM users and click on your new user which is highlighted in blue.  Select the 'create access key' choice on the right.  On the "Access key best practices & alternatives" screen select the "Other" choice and click next.  On the next screen press the 'Create Access Key'.  Write down in a secure place your AWS access key and your AWS secret access key which you will need later.

## Single Region Setup

1. Create your Virtual Private Cloud.  Go to the VPC screen and make sure you have selected the AWS region that you want the VPC created in which is also where your database cluster will be located in.  Select the 'create VPC' button in the upper right.  Then do as follows:

   1. Select the' VPC and more' radio button.
   2. Under the name tag, clear the default entry of 'Project' and put in the name you want for your VPC and its related objects.  I used 'adam-london'.
   3. Record your CIDR block for later use.  You can leave the default of 10.0.0.0/16
   4. Change your 'Number of Availability Zones' to 3 and it will automatically update the number of private and public subnets.
   5. Take a screen snapshot of the 'Preview screen' on your right.  You'll need the names of your VPC and public and private subnets for later use.
   6. Create your VPC.

Here is a picture of my VPC preview screen:

![AWS VPC Sample Configuration](https://github.com/adamblondon01/dbworkload-demo/blob/859a7920f5c60df1a7c78d82dde2cf79217c4548/sample%20AWS%20VPC%20configuration.png)

2. Create your internal and external security groups.  Go to the 'Security Groups' screen and make sure you have selected the AWS region that you want these security groups created in, and correspondingly, your VPC and database cluster. (Look in the upper right corner for the region dropdown list.)

   1. Create the external security group.  This will be used by your app server node that is in the same region as the database cluster, and only your two app server nodes will have access to the internet.  Make sure you have inbound entries for ports 22, 8080 and 26257 for your home IP address and the Netskope IP addresses.  (See [Netskope IP Addresses](https://cockroachlabs.atlassian.net/wiki/spaces/HELP/pages/3735027747/Netskope+IP+Ranges) )  Make sure each IP address has a '/32' at the end of each one so that only that specific IP address can have access.  For your outbound entries, you can enable all ports to access all IP addresses. ( IP address 0.0.0.0/0 )

   2. Create your internal security group.  This will be used by your database cluster nodes and they will not have access to the internet.  You'll need inbound entries for ports 22, 8080 and 26257.  For the IP address use your VPC block fo 10.0.0.0/16 to allow inbound connections only from addresses inside your VPC (i.e. allow only database cluster nodes and the app server node to talk with each other.)  For your outbound entries you can enable everything.

   3. Create an additional external security group for your remote application server.  This node will also have access to the internet.  Pick a region on the other side of the country from your database cluster.  In my case I used us-west-1 since I created my database cluster in us-east-2.  For your inbound entries, you just need an entry for port 22 for your home IP address and the Netskope IP addresses.  For your outbound entries, you can enable all ports to access all IP addresses.

3. Create your EC2 instances.  I won't cover everything; just the key points.  Start with the app server that you want in the same region as the database cluster.
   1. Make sure you are in the same region as your VPC.
   2. Press the 'Launch Instances' button.
   3. I suggest you use the AWS Linux 2023 AMI, which is the default.
   4. For your instance type I would suggest a m6a.xlarge or a m6a.2xlarge depending on what you're going to be running on the app server.
   5. Choose the key pair for this region that you created earlier.
   6. Edit your network settings and choose the VPC you already created and 1 of the three public subnets you created.  (It doesn't matter which one just that it is public.)
   7. Enable the 'Auto-assign public' ip address setting
   8. Choose the 'Select existing security group' radio button and choose the external security group you created earlier.
   9. Under 'Configure storage' 100GB of gp3 storage should be a good size.
   10. Launch the instance.

4. For the 3 database cluster nodes repeat step 3 except as follows:
   1. I suggest an instance type of m6a.2xlarge
   2. Each node should be on a different private subnet zone (so one node on private zone a, one node on private zone b, and one node on private zone c)
   3. Disable the 'Auto-assign public' ip address setting
   4. For the security group choose the internal security group you created earlier

5. For the remote app server node:
   1. Choose the region where you created the remote external security group.
   2. Follow the steps in #3 except choose the roachpod vpc and one of the roachpod's public subnets.  There's no reason to create a new VPC unless you want to. :-)

6. Copy the .pem file for your database cluster region to the app server in the region.  Since you can access this node from the internet, you'll use this node to connect to the database nodes to setup them up.

7. Download CockroachDB

   1. Download the CockroachDB full executable to each of the database nodes.  (Intel version if you're using m6a EC2 instances - if you're not sure run the uname -p Linux command)
  
      I suggest you use curl on your app server node or your Mac's browser and then push the files to each of the database nodes using sftp or scp.  If you're on your Mac, you'll need to push the file first to the app server node and then log onto the app server node and push it to each of the database nodes.

   [Link to Cockroach Downloads](https://www.cockroachlabs.com/docs/releases/v24.2)

   2. Download the CocckroachDB SQL only executable and push it to each of the app server nodes.

8. Configure the database and app server nodes

9. Setup your database and client certificates.  (You'll need client certs for both app server nodes.)  Don't forget to include your app servers private IP addresses.  I suggest you do the certs creation on the app server that is in the database cluster region.  See:

   [how to create your certificates](https://www.cockroachlabs.com/docs/stable/manage-certs-cli)
   
   [Certification creation on AWS](https://www.cockroachlabs.com/docs/stable/deploy-cockroachdb-on-aws#step-5-generate-certificates)

11. Start your database.  I suggest you write a bash script to do this on the app server in your database region.

12. Setup and start HAProxy on both app server nodes.  You can use the cockroach gen command to help.

    [Cockroach gen reference](https://www.cockroachlabs.com/docs/v24.2/cockroach-gen.html)

    For an example of a working HAProxy server configuration file see (Note the use of the private IP addresses):

    ![Example HAProxy Configuration File](https://github.com/adamblondon01/dbworkload-demo/blob/176cfcebacf634d538d53e3fe3ad9063fe08e9b6/haproxy.cfg)

### Single Region Application Setup

1. Install dbworkload on your app servers.  See Fabio's Github for details:

   [dbworkload Details](https://fabiog1901.github.io/dbworkload/)

2. Copy all the files listed above except for haproxy.cfg to the app server in the database cluster region.  Copy the ulta_beauty.py and the run_dbworkload.sh files to the app server in the remote region.

__Run the remaining setup steps on the app server in the database cluster region:__

3. Build the tables and the test tables by running the database_build_script.sql and the build_test_tables.sql

4. Test the importing of customer and loyalty data by running the load_customers_test_aws.sql script and the load_loyalty_test_aws.sql script.  I have already placed the data files you'll need in a pair of S3 buckets.  First edit each file and enter in your access key and secret access key, which you created in step 3 of the general setup, and the S3 bucket's AWS region name, us-east-2, where I already loaded the data.  When run, the data will be loaded into the customers_test and loyalty_test tables, which you can check.  This will confirm that you correctly setup your S3 connectivity.  If you want to confirm against the original data, please see the customers_test.csv and loyalty_test.csv files, which I've also added to the list of files attached to this repository.  If you have any issues you may want to configure the aws cli and run the following command which will show you a list of all the S3 buckets and let you know if you have connectivity from the app server node to S3.

   ```
   aws s3 ls
   ```

   If you wish you can also build your own version of the 2 csv files by running dbworkload util with the customers_test.yaml and loyalty_test.yaml as follows:

   ```
   dbworkload util csv -i customers_test.yaml -x 1 -d ','
   dbworkload util csv -i loyalty_test.yaml -x 1 -d ','
   ```

   Ignore the output from dbworkload since it isn't relevant for this demo.  dbworkload will create 2 sub-directories, customer_test and loyalty_test, with the data for each run in each sub-directory.  You'll need to place these 2 data files in the S3 bucket you created in the general setup step #2 by using the 'aws s3 cp' command, alter the 2 SQL scripts to reflect your new S3 bucket name (and possibly region) and then run the SQL scripts.

   To understand all the dbworkload util csv parameters, run:

   ```
   dbworkload util csv --help
   ```

5. Import the data into your tables.  I have already copied the data into a pair of S3 buckets in region us-east-2 .  Edit the 2 import files, load_customers_aws.sql and load_loyalty_aws.sql, and put in your access key and secret access key and the region us-east-2 on all 300 lines in each file.  (There are 300 import files for each table; each with 100,000 rows of data.)  Run both scripts; be aware it will take a while.

   If you wish to regenerate the data for yourself, see the included customers.yaml and loyalty.yaml files, and see step 4 above.

6. Run the add_customers_keys_to_loyalty_table.sql .  It will take awhile.  This will setup the referential integrity between the 2 tables, customers and loyalty.

# Running your single region tests

To run your tests, run the run_dbworkload.sh.
